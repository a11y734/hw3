from pathlib import Path
from typing import Optional
import sys

import pandas as pd
import streamlit as st

REPO_ROOT = Path(__file__).resolve().parents[1]
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))

from spam_pipeline import artifacts, features, visualizations, preprocessing

st.set_page_config(page_title="ğŸ“§ åƒåœ¾éƒµä»¶æ™ºæ…§åˆ†æ", layout="wide")

# é è¨­æ”¹ç‚ºæŒ‡å‘è³‡æ–™å¤¾ï¼Œæœƒè‡ªå‹•å°‹æ‰¾æœ€åˆé©çš„æª”æ¡ˆ
DEFAULT_DATASET = Path("datasets")
DEFAULT_MODELS = Path("models")
DATASET_SOURCE_URL = (
    "https://github.com/PacktPublishing/Hands-On-Artificial-Intelligence-for-Cybersecurity"
    "/tree/master/Chapter03/datasets"
)


@st.cache_data(show_spinner=False)
def load_dataset(path: str, _mtime: float | None = None) -> pd.DataFrame:
    """è®€å– CSVï¼Œè‹¥åµæ¸¬åˆ°æ²’æœ‰è¡¨é ­å‰‡è‡ªå‹•è£œä¸Š label/textã€‚"""
    needs_header = False
    try:
        with open(path, "r", encoding="utf-8") as fh:
            first_line = fh.readline().strip().lower()
        prefix = first_line.replace('"', "").split(",", 1)[0]
        if prefix in {"ham", "spam"}:
            needs_header = True
    except OSError:
        pass

    if needs_header:
        df = pd.read_csv(path, header=None, names=["label", "text"], dtype=str)
    else:
        df = pd.read_csv(path)
    return df


def normalize_text_for_model(text_value: str, target_col: str) -> str:
    """è‹¥æ¨¡å‹ä½¿ç”¨ *_clean æ¬„ä½ï¼Œå¥—ç”¨é è¨­å‰è™•ç†ä»¥è²¼è¿‘è¨“ç·´è³‡æ–™ã€‚"""
    if not text_value:
        return ""
    if "clean" in target_col.lower():
        cleaned, _ = preprocessing.preprocess_text_column(
            pd.Series([text_value]),
            preprocessing.PreprocessConfig(),
        )
        return cleaned.iloc[0]
    return text_value


@st.cache_resource(show_spinner=False)
def load_model(models_dir: str):
    bundle = artifacts.ArtifactBundle(Path(models_dir))
    pipeline = bundle.load("spam_pipeline")
    metadata = bundle.load_metadata()
    return pipeline, metadata


def resolve_dataset_path(path: Path) -> Optional[Path]:
    """å…è¨±è¼¸å…¥æª”æ¡ˆæˆ–è³‡æ–™å¤¾ï¼›è‹¥ç‚ºè³‡æ–™å¤¾å„ªå…ˆæŒ‘é¸ raw/label+text CSVã€‚"""
    if path.is_file() and path.suffix.lower() == ".csv":
        return path
    if path.is_dir():
        candidates = list(path.rglob("*.csv"))
        if not candidates:
            return None
        def candidate_score(p: Path) -> tuple[int, int]:
            parts_lower = {part.lower() for part in p.parts}
            name_lower = p.name.lower()
            score = 0
            if "processed" in parts_lower:
                score += 4
            if "steps" in parts_lower:
                score += 5
            if "clean" in name_lower:
                score += 2
            if "full" in name_lower:
                score += 1
            if "raw" in parts_lower or "raw" in name_lower:
                score -= 2
            if "no_header" in name_lower or "original" in name_lower:
                score -= 3
            if "spam" in name_lower and "clean" not in name_lower:
                score -= 1
            try:
                size = p.stat().st_size
            except OSError:
                size = 0
            return (score, -int(size))

        return min(candidates, key=candidate_score)
    return None


def pick_dataset(local_path: Path) -> tuple[Optional[pd.DataFrame], Optional[Path]]:
    resolved = resolve_dataset_path(local_path)
    if resolved and resolved.exists():
        try:
            mtime = resolved.stat().st_mtime
        except OSError:
            mtime = None
        return load_dataset(str(resolved), mtime), resolved
    return None, None


def main():
    st.title("ğŸ“§ åƒåœ¾éƒµä»¶æ™ºæ…§åˆ†æå¥—ä»¶")
    st.write(
        "ä»¥ Packt ç¬¬ä¸‰ç« çš„åƒåœ¾éƒµä»¶ç¯„ä¾‹ç‚ºéˆæ„Ÿï¼Œæä¾›è³‡æ–™é è™•ç†ã€è©•ä¼°èˆ‡è¦–è¦ºåŒ–æµç¨‹ï¼Œ"
        "ä¸¦æ”¯æ´ CLI èˆ‡ Streamlit é›™ä»‹é¢ã€‚"
    )

    st.sidebar.header("è³‡æ–™ä¾†æº")
    dataset_path = st.sidebar.text_input("è³‡æ–™é›†è·¯å¾‘", str(DEFAULT_DATASET))
    data, resolved_path = pick_dataset(Path(dataset_path))
    if data is None:
        st.warning("æ‰¾ä¸åˆ°è³‡æ–™é›†ï¼Œè‡³å°‘å…ˆåŸ·è¡Œé è™•ç†è…³æœ¬æˆ–ä¸Šå‚³ CSVã€‚")
        return
    if data.empty:
        st.warning("è³‡æ–™é›†ç‚ºç©ºï¼Œè«‹æä¾›è‡³å°‘ 1 ç­†æ¨£æœ¬ã€‚")
        return

    # è‡ªå‹•æ¨æ¸¬æ¬„ä½é è¨­
    columns = list(data.columns)
    label_candidates = ["label", "category", "target", "class", "y"]
    text_candidates = ["text_clean", "text", "message", "sms", "v2"]
    def_idx_label = next((i for i, c in enumerate(columns) if c.lower() in label_candidates), 0)
    text_options = [c for c in columns if data[c].dtype == object]
    def_idx_text = next((i for i, c in enumerate(text_options) if c.lower() in text_candidates), 0 if text_options else 0)

    label_col = st.sidebar.selectbox("æ¨™ç±¤æ¬„ä½", options=columns, index=def_idx_label)
    text_col = st.sidebar.selectbox("æ–‡å­—æ¬„ä½", options=text_options or columns, index=def_idx_text)
    st.sidebar.subheader("è³‡æ–™é è¦½")
    st.sidebar.caption("å›ºå®šé¡¯ç¤ºå‰ 5 ç­†ã€‚")
    preview_rows = min(5, int(len(data)))
    max_top_tokens = int(len(data))

    st.subheader("è³‡æ–™æ¦‚æ³")
    table_height = min(700, 38 * preview_rows + 60)
    st.dataframe(data.head(preview_rows), use_container_width=True, height=table_height)
    st.caption(
        f"æ¨£æœ¬æ•¸ï¼š{len(data)} ï¼è³‡æ–™ä¾†æºï¼š[Packt Hands-On AI for Cybersecurity]"
        f"({DATASET_SOURCE_URL}) â†’ `{resolved_path or Path(dataset_path)}`"
    )
    st.caption("è³‡æ–™è¡¨åƒ…é¡¯ç¤ºå‰ N ç­†ï¼Œæ‰€æœ‰çµ±è¨ˆèˆ‡åœ–è¡¨ä»ä½¿ç”¨å®Œæ•´è³‡æ–™é›†ã€‚")

    label_series = data[label_col].astype(str)
    counts = label_series.value_counts()
    label_order = counts.index.tolist()
    spam_label_name = next((lbl for lbl in label_order if lbl.lower() == "spam"), label_order[0])
    ham_label_name = (
        next((lbl for lbl in label_order if lbl.lower() == "ham"), label_order[1] if len(label_order) > 1 else label_order[0])
        if label_order
        else "ham"
    )
    if ham_label_name == spam_label_name and len(label_order) > 1:
        ham_label_name = next((lbl for lbl in label_order if lbl != spam_label_name), ham_label_name)
    col1, col2 = st.columns(2)
    with col1:
        st.metric(f"åƒåœ¾éƒµä»¶ï¼ˆ{spam_label_name}ï¼‰æ•¸é‡", int(counts.get(spam_label_name, 0)))
    with col2:
        st.metric(f"æ­£å¸¸éƒµä»¶ï¼ˆ{ham_label_name}ï¼‰æ•¸é‡", int(counts.get(ham_label_name, 0)))
    st.caption(
        f"åˆ†é¡ç¸½æ•¸ï¼š{int(counts.sum())}ï¼ˆ= å…¨éƒ¨æ¨£æœ¬ {len(data)} ç­†ï¼‰ã€‚è‹¥ä¸ç›¸ç¬¦è«‹æª¢æŸ¥æ¨™ç±¤æ¬„ä½ã€‚"
    )

    st.subheader("è¦–è¦ºåŒ–")
    fig = visualizations.plot_class_distribution(data, label_col)
    chart_col, info_col = st.columns([3, 2], gap="small")
    with chart_col:
        st.pyplot(fig, use_container_width=True)
    with info_col:
        st.caption("å„é¡åˆ¥æ¨£æœ¬æ•¸ï¼ˆå¯æ­é…åœ–è¡¨è§€å¯Ÿæ¯”ä¾‹ï¼‰")
        summary_df = counts.rename_axis("æ¨™ç±¤").reset_index(name="æ•¸é‡")
        st.dataframe(
            summary_df,
            use_container_width=True,
            height=min(200, 38 * len(summary_df) + 38),
        )

    st.markdown("### Top Tokens by Class")
    st.caption("ä½¿ç”¨ä¸‹æ–¹ Top-N slider èª¿æ•´æ¯å€‹é¡åˆ¥é¡¯ç¤ºçš„ tokens æ•¸é‡ï¼ˆæœ€å¤š 100ï¼‰ã€‚")
    token_slider_max = max(1, min(100, max_top_tokens))
    token_slider_min = 5 if token_slider_max >= 5 else 1
    token_slider_step = 5 if token_slider_max >= 15 else 1
    default_top_token_limit = min(20, token_slider_max)
    stored_limit = st.session_state.get("top_token_slider", default_top_token_limit)
    top_token_limit = int(
        max(token_slider_min, min(token_slider_max, stored_limit))
    )
    top_tokens = features.top_tokens_by_class(data, label_col, text_col, topn=int(top_token_limit))
    non_empty = {label: items for label, items in top_tokens.items() if items}
    if non_empty:
        columns = st.columns(len(non_empty))
        for (label, items), column in zip(non_empty.items(), columns):
            column.plotly_chart(
                plot_top_tokens_by_label(label, items),
                use_container_width=True,
            )
    else:
        st.info("è©å½™çµ±è¨ˆä¸è¶³ï¼Œè«‹ç¢ºèªæ–‡å­—æ¬„ä½æ˜¯å¦å­˜åœ¨ã€‚")
    st.slider(
        "Top-N tokensï¼ˆæ¯é¡åˆ¥ï¼‰",
        min_value=token_slider_min,
        max_value=token_slider_max,
        value=top_token_limit,
        step=token_slider_step,
        help="èª¿æ•´æ¯å€‹é¡åˆ¥é¡¯ç¤ºçš„ tokens æ•¸é‡ï¼Œä¸Šé™ 100ã€‚",
        key="top_token_slider",
    )

    st.subheader("æ¨¡å‹ç‹€æ…‹")
    models_dir = st.sidebar.text_input("æ¨¡å‹ç›®éŒ„", str(DEFAULT_MODELS))
    model_text_col = None
    try:
        pipeline, metadata = load_model(models_dir)
        metrics = metadata.get("metrics", {})
        model_text_col = metadata.get("text_col")
        st.write(
            {
                "æ­£é¡åˆ¥": metadata.get("positive_label"),
                "æ±ºç­–é–¾å€¼": metrics.get("threshold"),
                "ç²¾ç¢ºç‡": metrics.get("precision"),
                "å¬å›ç‡": metrics.get("recall"),
                "F1": metrics.get("f1"),
                "æ–‡å­—æ¬„ä½ï¼ˆæ¨¡å‹ï¼‰": model_text_col,
            }
        )
        if model_text_col and model_text_col != text_col:
            st.info(
                f"æ¨¡å‹ä½¿ç”¨çš„æ–‡å­—æ¬„ä½ç‚º `{model_text_col}`ï¼Œèˆ‡å´æ¬„é¸å–çš„ `{text_col}` ä¸åŒï¼›æ¨è«–æœƒè‡ªå‹•æ”¹ç”¨æ¨¡å‹æ¬„ä½ã€‚"
            )
    except Exception as exc:  # noqa: BLE001
        st.info(f"è¼‰å…¥æ¨¡å‹å¤±æ•—æˆ–å°šæœªè¨“ç·´ï¼š{exc}")
        pipeline = None
        metadata = None

    st.subheader("å³æ™‚æ¨è«–")

    def random_example_from_label(target_label: str) -> Optional[str]:
        matches = data[label_series == target_label]
        if matches.empty or text_col not in matches.columns:
            return None
        return str(matches.sample(1)[text_col].iloc[0])

    col_spam, col_ham = st.columns(2)
    with col_spam:
        if st.button("éš¨æ©Ÿåƒåœ¾éƒµä»¶ç¯„ä¾‹"):
            example = random_example_from_label(spam_label_name)
            if example:
                st.session_state["candidate_text"] = example
            else:
                st.warning("è³‡æ–™é›†ä¸­æ‰¾ä¸åˆ°åƒåœ¾éƒµä»¶æ¨£æœ¬ã€‚")
    with col_ham:
        if st.button("éš¨æ©Ÿæ­£å¸¸éƒµä»¶ç¯„ä¾‹"):
            example = random_example_from_label(ham_label_name)
            if example:
                st.session_state["candidate_text"] = example
            else:
                st.warning("è³‡æ–™é›†ä¸­æ‰¾ä¸åˆ°æ­£å¸¸éƒµä»¶æ¨£æœ¬ã€‚")

    text_value = st.text_area("è¼¸å…¥è¨Šæ¯", value=st.session_state.get("candidate_text", ""))
    threshold = st.slider(
        "é–¾å€¼",
        min_value=0.1,
        max_value=0.9,
        value=float(metadata.get("metrics", {}).get("threshold", 0.5) if metadata else 0.5),
        step=0.01,
    )

    if st.button("é æ¸¬"):
        if pipeline is None:
            st.error("å°šæœªè¼‰å…¥æ¨¡å‹ï¼Œè«‹å…ˆè¨“ç·´æˆ–æ”¾ç½® artifactsã€‚")
        else:
            numeric_cols = metadata.get("numeric_cols", []) if metadata else []
            inference_text_col = model_text_col or text_col
            inference_df = build_inference_frame(text_value, inference_text_col, numeric_cols)
            prob = float(pipeline.predict_proba(inference_df)[:, 1])
            positive_label = metadata.get("positive_label", "spam")
            prob_pct = prob * 100
            if positive_label.lower() == "spam":
                positive_display = "åƒåœ¾éƒµä»¶"
                negative_display = "æ­£å¸¸éƒµä»¶"
            else:
                positive_display = positive_label
                negative_display = f"é {positive_label}"
            is_positive = prob >= threshold
            label = positive_display if is_positive else negative_display
            st.success(f"é æ¸¬çµæœï¼š{label}")
            st.caption(
                f"æ¨¡å‹åˆ¤ç‚º `{positive_label}` çš„æ©Ÿç‡ï¼š{prob_pct:.1f}%ï¼ˆé–¾å€¼ {threshold:.2f}ï¼‰"
            )
            st.progress(min(max(prob, 0), 1))


def build_inference_frame(text_value: str, text_col: str, numeric_cols: list[str]) -> pd.DataFrame:
    """å»ºç«‹å–®ç­†æ¨è«–è³‡æ–™ï¼Œç¢ºä¿æ¬„ä½ç¬¦åˆè¨“ç·´æµç¨‹ã€‚"""
    normalized_text = normalize_text_for_model(text_value, text_col)
    base = pd.DataFrame({text_col: [normalized_text]})
    if numeric_cols:
        derived = features.derive_text_features(base[text_col], prefix=text_col)
        for col in numeric_cols:
            if col not in derived.columns:
                derived[col] = 0.0
        base = pd.concat([base, derived[numeric_cols]], axis=1)
    return base


def plot_top_tokens_by_label(label: str, items: list[tuple[str, int]]):
    import plotly.express as px

    df = pd.DataFrame(items, columns=["token", "count"])
    fig = px.bar(
        df,
        x="count",
        y="token",
        orientation="h",
        title=f"Class: {label}",
        labels={"count": "æ¬¡æ•¸", "token": "è©å½™"},
    )
    fig.update_layout(height=320, margin=dict(l=20, r=20, t=40, b=20))
    return fig


if __name__ == "__main__":
    main()
